{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresPlazas19931504/PROYECTOINTEGRADOR-PREICA2501B020071/blob/main/S20_Evidencia_de_Aprendizaje_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNMQh6tJQ0SY"
      },
      "source": [
        "#**S25 - Evidencia de Aprendizaje 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y7WLzEOQ5w5"
      },
      "source": [
        "##INSTALACIONES Y CONFIGURACIÓN INICIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afEUH6S4P6wS"
      },
      "outputs": [],
      "source": [
        "# Instala las librerías necesarias KaggleHub y ydata-profiling\n",
        "# --upgrade --quiet asegura que se actualicen y no muestren la salida detallada\n",
        "!pip install kagglehub[pandas-datasets] ydata-profiling --upgrade --quiet\n",
        "\n",
        "# Importa la librería pandas para manipulación de datos\n",
        "import pandas as pd\n",
        "# Importa kagglehub para cargar datasets de Kaggle\n",
        "import kagglehub\n",
        "# Importa KaggleDatasetAdapter para especificar el tipo de adaptador (Pandas en este caso)\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "# Importa ProfileReport de ydata-profiling para generar reportes exploratorios\n",
        "from ydata_profiling import ProfileReport\n",
        "# Importa numpy para operaciones numéricas (aunque no se usa directamente en esta celda, es una importación común)\n",
        "import numpy as np\n",
        "# Importa display para mostrar DataFrames de forma más amigable en Colab\n",
        "from IPython.display import display\n",
        "# Importa warnings para manejar advertencias\n",
        "import warnings\n",
        "# Ignora las advertencias para no saturar la salida\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura pandas para mostrar todas las columnas de un DataFrame\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Configura pandas para mostrar números flotantes con 2 decimales\n",
        "pd.set_option('display.precision', 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K8eDKfXRvb-"
      },
      "source": [
        "## CARGA Y OPTIMIZACIÓN DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqUQwPUoR1qf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def load_dataset_fast(file_path=\"2019-Oct.csv\", sample_size=500000):\n",
        "    \"\"\"Carga dataset de forma ultra-rápida con optimizaciones automáticas y muestreo.\"\"\"\n",
        "\n",
        "    # Tipos optimizados - using the most efficient ones\n",
        "    dtype_dict = {\n",
        "        'event_type': 'category',\n",
        "        'product_id': 'int32',\n",
        "        'category_id': 'int64',\n",
        "        'category_code': 'category',\n",
        "        'brand': 'category',\n",
        "        'price': 'float32',\n",
        "        'user_id': 'int32',\n",
        "        'user_session': 'category'\n",
        "    }\n",
        "\n",
        "    print(f\"Cargando: {file_path}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # OPTIMIZACIÓN 1: Cargar con tipos desde el inicio\n",
        "        # kagglehub.load_dataset does not have a dtype parameter, removing it.\n",
        "        df = kagglehub.load_dataset(\n",
        "            KaggleDatasetAdapter.PANDAS,\n",
        "            \"mkechinov/ecommerce-behavior-data-from-multi-category-store\",\n",
        "            file_path,\n",
        "            # dtype=dtype_dict # Removed as it's not supported\n",
        "        )\n",
        "        print(\"Cargado con tipos optimizados\")\n",
        "\n",
        "    except:\n",
        "        # OPTIMIZACIÓN 2: Carga rápida + conversión eficiente\n",
        "        df = kagglehub.load_dataset(\n",
        "            KaggleDatasetAdapter.PANDAS,\n",
        "            \"mkechinov/ecommerce-behavior-data-from-multi-category-store\",\n",
        "            file_path\n",
        "        )\n",
        "\n",
        "        # Conversión rápida por lotes\n",
        "        print(\"Optimizando tipos...\")\n",
        "\n",
        "        # Convertir categóricas (más rápido en lote)\n",
        "        categorical_cols = ['event_type', 'category_code', 'brand', 'user_session']\n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype('category')\n",
        "\n",
        "        # Convertir numéricos (downcast automático)\n",
        "        numeric_cols = {'product_id': 'int32', 'user_id': 'int32', 'price': 'float32'}\n",
        "        for col, dtype in numeric_cols.items():\n",
        "            if col in df.columns:\n",
        "                # Removed downcast argument as it was causing the error\n",
        "                df[col] = pd.to_numeric(df[col], errors='ignore')\n",
        "                # Convert to specific dtype after to_numeric\n",
        "                df[col] = df[col].astype(dtype)\n",
        "\n",
        "    # OPTIMIZACIÓN 3: Muestreo si el dataset es más grande que el sample_size\n",
        "    if len(df) > sample_size:\n",
        "        print(f\"Dataset grande ({len(df):,} filas), tomando muestra de {sample_size:,}\")\n",
        "        # Muestreo estratificado por 'event_type' si es posible\n",
        "        if 'event_type' in df.columns and df['event_type'].nunique() > 1:\n",
        "             df = df.groupby('event_type', group_keys=False).apply(\n",
        "                lambda x: x.sample(n=min(len(x), sample_size // df['event_type'].nunique()), random_state=42)\n",
        "            ).reset_index(drop=True)\n",
        "        else:\n",
        "            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "    else:\n",
        "        print(f\"Dataset pequeño ({len(df):,} filas), usando dataset completo\")\n",
        "\n",
        "\n",
        "    # OPTIMIZACIÓN 4: Stats rápidos\n",
        "    elapsed = time.time() - start_time\n",
        "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "    print(f\" Dataset cargado y muestreado en {elapsed:.1f}s:\")\n",
        "    print(f\"  • Filas: {len(df):,}\")\n",
        "    print(f\"  • Columnas: {len(df.columns)}\")\n",
        "    print(f\"  • Memoria: {memory_mb:.1f} MB\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Uso ultra-simple con sample_size=500000:\n",
        "df = load_dataset_fast(sample_size=500000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9EWDmdnSHht"
      },
      "source": [
        "## ANÁLISIS EXPLORATORIO BÁSICO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p91EaMu-SMyf"
      },
      "outputs": [],
      "source": [
        "# Muestra las primeras 5 filas del DataFrame para una vista previa\n",
        "display(df.head())\n",
        "\n",
        "# Muestra información sobre el DataFrame, incluyendo tipos de datos, valores no nulos y uso de memoria (calculado profundamente)\n",
        "df.info(memory_usage='deep')\n",
        "\n",
        "# Muestra estadísticas descriptivas del DataFrame.\n",
        "# include='all' asegura que se incluyan estadísticas para columnas numéricas y categóricas.\n",
        "display(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOoAu6ATSjA8"
      },
      "source": [
        "## ANÁLISIS DE VALORES ÚNICOS Y NULOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8UuWwVXSpS-"
      },
      "outputs": [],
      "source": [
        "# Inicializa un diccionario para almacenar el conteo de valores únicos por columna\n",
        "unique_counts = {}\n",
        "# Itera sobre cada columna del DataFrame\n",
        "for col in df.columns:\n",
        "    # Calcula el número de valores únicos para la columna actual y lo almacena en el diccionario\n",
        "    unique_counts[col] = df[col].nunique()\n",
        "    # Imprime el nombre de la columna y el número de valores únicos, formateado con comas\n",
        "    print(f\"   • {col}: {unique_counts[col]:,} valores únicos\")\n",
        "\n",
        "# Calcula el número de valores nulos para cada columna\n",
        "null_counts = df.isnull().sum()\n",
        "# Comprueba si hay alguna columna con valores nulos (si la suma de nulos es mayor que 0)\n",
        "if null_counts.sum() > 0:\n",
        "    # Si hay nulos, imprime un encabezado\n",
        "    print(\"Columnas con valores nulos:\")\n",
        "    # Itera sobre las columnas que tienen valores nulos (donde el conteo es mayor que 0)\n",
        "    for col, count in null_counts[null_counts > 0].items():\n",
        "        # Imprime el nombre de la columna, el número de valores nulos y su porcentaje respecto al total de filas\n",
        "        print(f\"• {col}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
        "# Si no hay valores nulos en ninguna columna\n",
        "else:\n",
        "    # Imprime un mensaje indicando que no hay valores nulos\n",
        "    print(\"No hay valores nulos en el dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48SZmvYeS5MM"
      },
      "source": [
        "## ANÁLISIS ESPECÍFICO E-COMMERCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXHEP5tlS-0K"
      },
      "outputs": [],
      "source": [
        "# Calcula la distribución de los tipos de eventos y la imprime\n",
        "event_dist = df['event_type'].value_counts()\n",
        "print(event_dist)\n",
        "# Imprime un encabezado para los porcentajes\n",
        "print(f\"\\nPorcentajes:\")\n",
        "# Itera sobre la distribución de eventos y calcula e imprime el porcentaje de cada tipo de evento\n",
        "for event, count in event_dist.items():\n",
        "    print(f\"   • {event}: {count/len(df)*100:.2f}%\")\n",
        "\n",
        "# Calcula los 10 productos más frecuentes (por conteo) y los imprime\n",
        "top_products = df['product_id'].value_counts().head(10)\n",
        "print(top_products)\n",
        "\n",
        "# Calcula las 10 marcas más frecuentes (por conteo) y las imprime\n",
        "top_brands = df['brand'].value_counts().head(10)\n",
        "print(top_brands)\n",
        "\n",
        "# Calcula estadísticas descriptivas para la columna 'price' y las imprime\n",
        "price_stats = df['price'].describe()\n",
        "print(price_stats)\n",
        "\n",
        "# Itera sobre los valores únicos en la columna 'event_type'\n",
        "for event_type in df['event_type'].unique():\n",
        "    # Filtra el DataFrame para obtener solo las filas del tipo de evento actual\n",
        "    event_prices = df[df['event_type'] == event_type]['price']\n",
        "    # Imprime el tipo de evento\n",
        "    print(f\"\\n{event_type}:\")\n",
        "    # Calcula e imprime el precio promedio para este tipo de evento\n",
        "    print(f\"• Promedio: ${event_prices.mean():.2f}\")\n",
        "    # Calcula e imprime la mediana del precio para este tipo de evento\n",
        "    print(f\"• Mediana: ${event_prices.median():.2f}\")\n",
        "    # Calcula e imprime el precio máximo para este tipo de evento\n",
        "    print(f\"• Máximo: ${event_prices.max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thFkrTHjTJCf"
      },
      "source": [
        "## PREPARACIÓN DE MUESTRA PARA REPORTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBRY94DcTV2M"
      },
      "outputs": [],
      "source": [
        "# Obtiene el número total de filas en el DataFrame\n",
        "total_rows = len(df)\n",
        "# Define un tamaño de muestra si el dataset es grande\n",
        "if total_rows > 100000:\n",
        "    sample_size = 50000 # Tamaño de muestra para datasets muy grandes\n",
        "    print(f\"Dataset grande ({total_rows:,} filas), usando muestra de {sample_size:,}\")\n",
        "# Define un tamaño de muestra si el dataset es mediano\n",
        "elif total_rows > 50000:\n",
        "    sample_size = 25000 # Tamaño de muestra para datasets medianos\n",
        "    print(f\"Dataset mediano ({total_rows:,} filas), usando muestra de {sample_size:,}\")\n",
        "# Si el dataset es pequeño, usa el dataset completo como muestra\n",
        "else:\n",
        "    sample_size = total_rows # Usa el total de filas si es pequeño\n",
        "    print(f\"Dataset pequeño ({total_rows:,} filas), usando dataset completo\")\n",
        "\n",
        "# Si el tamaño de la muestra es menor que el total de filas (es decir, si se necesita muestreo)\n",
        "if sample_size < total_rows:\n",
        "    # Crea una muestra estratificada por tipo de evento.\n",
        "    # Agrupa el DataFrame por 'event_type'.\n",
        "    # Aplica una función lambda a cada grupo para tomar una muestra.\n",
        "    # El tamaño de la muestra para cada grupo es el mínimo entre el tamaño del grupo y el tamaño total de la muestra dividido por el número de tipos de eventos únicos.\n",
        "    # random_state=42 asegura reproducibilidad.\n",
        "    # reset_index(drop=True) reinicia el índice y elimina la columna de índice original.\n",
        "    df_muestra = df.groupby('event_type', group_keys=False).apply(\n",
        "        lambda x: x.sample(n=min(len(x), sample_size//len(df['event_type'].unique())),\n",
        "                          random_state=42)\n",
        "    ).reset_index(drop=True)\n",
        "# Si no se necesita muestreo (dataset pequeño), la muestra es una copia del DataFrame original\n",
        "else:\n",
        "    df_muestra = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMMi6HprThnA"
      },
      "source": [
        "## GENERACIÓN DE REPORTE YDATA-PROFILING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWey3xvTTlnJ"
      },
      "outputs": [],
      "source": [
        "# Define un diccionario de configuración para el reporte de ydata-profiling.\n",
        "# 'minimal': True genera un reporte más rápido y básico.\n",
        "# 'interactions': {'continuous': False} desactiva el análisis de interacciones entre variables continuas.\n",
        "# 'correlations': {'auto': {'calculate': False}} desactiva el cálculo de correlaciones.\n",
        "# 'missing_diagrams': desactiva los diagramas de valores faltantes.\n",
        "# 'duplicates': {'head': 0} desactiva la visualización de filas duplicadas.\n",
        "# 'samples': {'head': 5, 'tail': 5} muestra las primeras y últimas 5 filas como ejemplo.\n",
        "config = {\n",
        "    'minimal': True,\n",
        "    'interactions': {'continuous': False},\n",
        "    'correlations': {'auto': {'calculate': False}},\n",
        "    'missing_diagrams': {'bar': False, 'matrix': False, 'heatmap': False},\n",
        "    'duplicates': {'head': 0},\n",
        "    'samples': {'head': 5, 'tail': 5}\n",
        "}\n",
        "\n",
        "# Crea una instancia de ProfileReport utilizando el DataFrame de muestra y la configuración definida.\n",
        "reporte = ProfileReport(\n",
        "    df_muestra, # DataFrame a analizar\n",
        "    title=\"E-commerce Behavior Data Analysis\", # Título del reporte\n",
        "    config_file=None, # No usa un archivo de configuración externo\n",
        "    **config # Desempaqueta el diccionario de configuración como argumentos\n",
        ")\n",
        "\n",
        "# Define el nombre del archivo de salida para el reporte HTML\n",
        "output_file = 'reporte_ecommerce_segmentado.html'\n",
        "# Imprime un mensaje indicando dónde se guardará el reporte\n",
        "print(f\"Guardando reporte como: {output_file}\")\n",
        "# Guarda el reporte en un archivo HTML\n",
        "reporte.to_file(output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq8XmjV0Tt1B"
      },
      "source": [
        "## RESUMEN FINAL Y CONCLUSIONES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zFv1dt4TzRR"
      },
      "outputs": [],
      "source": [
        "# Imprime un resumen del dataset, incluyendo el número total de filas y columnas\n",
        "print(f\"Dataset: {total_rows:,} filas × {df.shape[1]} columnas\")\n",
        "# Imprime el rango de fechas cubierto por el dataset (desde el evento más antiguo hasta el más reciente)\n",
        "print(f\"Período: {df['event_time'].min()} a {df['event_time'].max()}\")\n",
        "# Imprime el número de usuarios únicos en el dataset\n",
        "print(f\"Usuarios únicos: {df['user_id'].nunique():,}\")\n",
        "# Imprime el número de productos únicos en el dataset\n",
        "print(f\"Productos únicos: {df['product_id'].nunique():,}\")\n",
        "# Imprime el número de marcas únicas en el dataset\n",
        "print(f\"Marcas únicas: {df['brand'].nunique():,}\")\n",
        "# Imprime el número de sesiones únicas en el dataset\n",
        "print(f\"Sesiones únicas: {df['user_session'].nunique():,}\")\n",
        "\n",
        "# Imprime un encabezado para el conteo de eventos por tipo\n",
        "print(f\"\\nEventos por tipo:\")\n",
        "# Itera sobre la distribución de los tipos de eventos y los imprime con su conteo y porcentaje\n",
        "for event, count in df['event_type'].value_counts().items():\n",
        "    print(f\"   • {event}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Imprime el valor promedio de los productos (basado en la columna 'price')\n",
        "print(f\"\\nValor promedio de productos: ${df['price'].mean():.2f}\")\n",
        "# Imprime el rango de precios (mínimo y máximo)\n",
        "print(f\"Rango de precios: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
        "\n",
        "# Imprime la ruta donde se guardó el reporte detallado\n",
        "print(f\"Reporte detallado disponible en: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL85jcnLUC9Z"
      },
      "source": [
        "## FUNCIONES ADICIONAL PARA ANÁLISIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5cRSaeAUPvN"
      },
      "outputs": [],
      "source": [
        "# Define una función para analizar la actividad de un usuario específico\n",
        "def analizar_usuario_especifico(user_id):\n",
        "    # Filtra el DataFrame para obtener solo los datos del usuario especificado\n",
        "    user_data = df[df['user_id'] == user_id]\n",
        "    # Si no se encuentra el usuario (el DataFrame filtrado está vacío)\n",
        "    if len(user_data) == 0:\n",
        "        # Imprime un mensaje indicando que el usuario no fue encontrado\n",
        "        print(f\"Usuario {user_id} no encontrado\")\n",
        "        # Sale de la función\n",
        "        return\n",
        "\n",
        "    # Imprime un encabezado para el análisis del usuario\n",
        "    print(f\"\\n--- ANÁLISIS USUARIO {user_id} ---\")\n",
        "    # Imprime el número total de eventos registrados para este usuario\n",
        "    print(f\"Total de eventos: {len(user_data)}\")\n",
        "    # Imprime un encabezado para los eventos por tipo\n",
        "    print(f\"Eventos por tipo:\")\n",
        "    # Imprime el conteo de cada tipo de evento para este usuario\n",
        "    print(user_data['event_type'].value_counts())\n",
        "    # Imprime el número de productos únicos que el usuario vio\n",
        "    print(f\"Productos únicos vistos: {user_data['product_id'].nunique()}\")\n",
        "    # Calcula e imprime el gasto total estimado del usuario (suma de precios de eventos de compra, aunque no se especifica el tipo de evento aquí, se asume un análisis general)\n",
        "    print(f\"Gasto total: ${user_data['price'].sum():.2f}\")\n",
        "    # Imprime un encabezado para las marcas favoritas\n",
        "    print(f\"Marcas favoritas:\")\n",
        "    # Imprime las 5 marcas más frecuentes interactuadas por el usuario\n",
        "    print(user_data['brand'].value_counts().head(5))\n",
        "\n",
        "# Define una función para analizar la actividad relacionada con un producto específico\n",
        "def analizar_producto_especifico(product_id):\n",
        "    # Filtra el DataFrame para obtener solo los datos relacionados con el producto especificado\n",
        "    product_data = df[df['product_id'] == product_id]\n",
        "    # Si no se encuentra el producto (el DataFrame filtrado está vacío)\n",
        "    if len(product_data) == 0:\n",
        "        # Imprime un mensaje indicando que el producto no fue encontrado\n",
        "        print(f\"Producto {product_id} no encontrado\")\n",
        "        # Sale de la función\n",
        "        return\n",
        "\n",
        "    # Imprime un encabezado para el análisis del producto\n",
        "    print(f\"\\n--- ANÁLISIS PRODUCTO {product_id} ---\")\n",
        "    # Imprime el número total de interacciones registradas para este producto\n",
        "    print(f\"Total de interacciones: {len(product_data)}\")\n",
        "    # Imprime el número de usuarios únicos que interactuaron con este producto\n",
        "    print(f\"Usuarios únicos: {product_data['user_id'].nunique()}\")\n",
        "    # Imprime el precio del producto (toma el precio de la primera fila encontrada)\n",
        "    print(f\"Precio: ${product_data['price'].iloc[0]:.2f}\")\n",
        "    # Imprime la marca del producto (toma la marca de la primera fila encontrada)\n",
        "    print(f\"Marca: {product_data['brand'].iloc[0]}\")\n",
        "    # Imprime la categoría del producto (toma la categoría de la primera fila encontrada)\n",
        "    print(f\"Categoría: {product_data['category_code'].iloc[0]}\")\n",
        "    # Imprime un encabezado para los eventos por tipo relacionados con el producto\n",
        "    print(f\"Eventos por tipo:\")\n",
        "    # Imprime el conteo de cada tipo de evento para este producto\n",
        "    print(product_data['event_type'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByLJRTWtWvOS"
      },
      "source": [
        "# **S20 - Evidencia de aprendizaje 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDJ4c_XVdHKj"
      },
      "source": [
        "## ELIMINACIÓN DE DUPLICADOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo-g2H2QyVoS"
      },
      "outputs": [],
      "source": [
        "# Almacena el número inicial de filas en el DataFrame\n",
        "initial_rows = len(df)\n",
        "# Elimina las filas duplicadas del DataFrame in-place (modifica el DataFrame directamente)\n",
        "df.drop_duplicates(inplace=True)\n",
        "# Almacena el número final de filas después de eliminar duplicados\n",
        "final_rows = len(df)\n",
        "# Calcula el número de duplicados que fueron eliminados\n",
        "duplicates_removed = initial_rows - final_rows\n",
        "\n",
        "# Imprime el número inicial de filas\n",
        "print(f\"Filas iniciales: {initial_rows:,}\")\n",
        "# Imprime el número final de filas\n",
        "print(f\"Filas finales: {final_rows:,}\")\n",
        "# Imprime el número de duplicados eliminados\n",
        "print(f\"Duplicados eliminados: {duplicates_removed:,}\")\n",
        "# Imprime un mensaje indicando si se eliminaron duplicados o no\n",
        "print(f\"¡Dataset limpio!\" if duplicates_removed > 0 else \"No había duplicados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyVbn_5edQFR"
      },
      "source": [
        "## TRATAMIENTO DE VALORES NULOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acb4d75a"
      },
      "outputs": [],
      "source": [
        "# Calcula el número de valores nulos por columna antes del tratamiento\n",
        "null_counts_before = df.isnull().sum()\n",
        "# Obtiene una lista de los nombres de las columnas que tienen valores nulos\n",
        "cols_with_nulls = null_counts_before[null_counts_before > 0].index.tolist()\n",
        "\n",
        "# Verifica si hay columnas con valores nulos\n",
        "if cols_with_nulls:\n",
        "    # Imprime un encabezado indicando las columnas con nulos antes del tratamiento\n",
        "    print(\"Columnas con valores nulos antes del tratamiento:\")\n",
        "    # Itera sobre las columnas con nulos\n",
        "    for col in cols_with_nulls:\n",
        "        # Obtiene el conteo de nulos para la columna actual\n",
        "        count = null_counts_before[col]\n",
        "        # Imprime el nombre de la columna, el conteo de nulos y su porcentaje\n",
        "        print(f\"• {col}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
        "\n",
        "    # Itera nuevamente sobre las columnas que tenían nulos para aplicar el tratamiento\n",
        "    for col in cols_with_nulls:\n",
        "        # Verifica si la columna actual todavía tiene valores nulos (importante si el tratamiento anterior no fue para todas las columnas)\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            # Si la columna es 'user_session'\n",
        "            if col == 'user_session':\n",
        "                # Almacena el número de filas antes de eliminar nulos en 'user_session'\n",
        "                initial_rows_session = len(df)\n",
        "                # Elimina las filas donde 'user_session' es nulo in-place\n",
        "                df.dropna(subset=[col], inplace=True)\n",
        "                # Almacena el número de filas después de eliminar nulos en 'user_session'\n",
        "                rows_after_dropping_session = len(df)\n",
        "                # Calcula el número de filas eliminadas debido a nulos en 'user_session'\n",
        "                dropped_session_rows = initial_rows_session - rows_after_dropping_session\n",
        "                # Imprime cuántas filas fueron eliminadas por nulos en 'user_session'\n",
        "                print(f\"Filas con valores nulos en '{col}' eliminadas: {dropped_session_rows}\")\n",
        "            # Si la columna no es 'user_session'\n",
        "            else:\n",
        "                # Calcula la moda (valor más frecuente) de la columna\n",
        "                mode_value = df[col].mode()[0]\n",
        "                # Rellena los valores nulos en la columna con la moda in-place\n",
        "                df[col].fillna(mode_value, inplace=True)\n",
        "                # Imprime un mensaje indicando que se imputaron los nulos con la moda\n",
        "                print(f\"Valores nulos en '{col}' imputados con la moda: '{mode_value}'\")\n",
        "\n",
        "# Si no había columnas con valores nulos inicialmente\n",
        "else:\n",
        "    # Imprime un mensaje indicando que no hay valores nulos\n",
        "    print(\"No hay valores nulos en el dataset\")\n",
        "\n",
        "# Calcula el número de valores nulos por columna después del tratamiento\n",
        "null_counts_after = df.isnull().sum()\n",
        "# Verifica si no quedan valores nulos (si la suma de nulos es 0)\n",
        "if null_counts_after.sum() == 0:\n",
        "    # Imprime un mensaje indicando que todos los nulos fueron tratados\n",
        "    print(\"\\n¡Todos los valores nulos han sido tratados!\")\n",
        "# Si todavía hay columnas con valores nulos\n",
        "else:\n",
        "    # Imprime un encabezado indicando las columnas restantes con nulos\n",
        "    print(\"\\nColumnas con valores nulos restantes:\")\n",
        "    # Itera sobre las columnas que todavía tienen nulos y las imprime con su conteo\n",
        "    for col, count in null_counts_after[null_counts_after > 0].items():\n",
        "        print(f\"• {col}: {count:,}\")\n",
        "\n",
        "# Muestra las primeras filas del DataFrame después del tratamiento de nulos\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AezlyERedYrD"
      },
      "source": [
        "## CONVERSIÓN DE TIPOS DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b4afd38"
      },
      "outputs": [],
      "source": [
        "# Convierte la columna 'event_time' al tipo de dato datetime de pandas.\n",
        "# Esto permite realizar operaciones basadas en tiempo.\n",
        "df['event_time'] = pd.to_datetime(df['event_time'])\n",
        "# Imprime un mensaje indicando que la conversión se realizó\n",
        "print(\"Convertido 'event_time' a objetos de datetime.\")\n",
        "\n",
        "# Define una lista de nombres de columnas que deberían ser de tipo categórico\n",
        "categorical_cols = ['event_type', 'category_code', 'brand', 'user_session']\n",
        "# Itera sobre la lista de columnas categóricas potenciales\n",
        "for col in categorical_cols:\n",
        "    # Verifica si la columna existe en el DataFrame y si su tipo de dato actual no es 'category'\n",
        "    if col in df.columns and df[col].dtype != 'category':\n",
        "        # Convierte la columna al tipo de dato 'category'\n",
        "        df[col] = df[col].astype('category')\n",
        "        # Imprime un mensaje indicando que la columna fue convertida\n",
        "        print(f\"Convertido '{col}' al tipo de dato categoría.\")\n",
        "\n",
        "# Imprime un encabezado para mostrar los tipos de datos después del ajuste\n",
        "print(\"\\nTipos de datos después del ajuste:\")\n",
        "# Muestra información sobre el DataFrame, incluyendo los tipos de datos actualizados y el uso de memoria (calculado profundamente)\n",
        "df.info(memory_usage='deep')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzqlgO7WdhaR"
      },
      "source": [
        "## IDENTIFICACIÓN DE COLUMNAS NUMÉRICAS Y ANÁLISIS DE OUTLIERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4a5378e"
      },
      "outputs": [],
      "source": [
        "# Inicializa una lista vacía para almacenar los nombres de las columnas numéricas\n",
        "numerical_cols = []\n",
        "# Itera sobre todas las columnas del DataFrame\n",
        "for col in df.columns:\n",
        "    # Verifica si el tipo de dato de la columna actual es uno de los tipos numéricos especificados\n",
        "    if df[col].dtype in ['int66', 'int32', 'float64', 'float32']:\n",
        "        # Si es un tipo numérico, agrega el nombre de la columna a la lista\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "# Imprime un encabezado indicando las columnas numéricas identificadas\n",
        "print(\"Columnas numéricas identificadas:\")\n",
        "# Imprime la lista de columnas numéricas\n",
        "print(numerical_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9354dae2"
      },
      "outputs": [],
      "source": [
        "# Importa las librerías necesarias para graficar: matplotlib.pyplot y seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Itera sobre la lista de columnas numéricas identificadas previamente\n",
        "for col in numerical_cols:\n",
        "    # Crea una nueva figura para cada boxplot con un tamaño específico\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Crea un boxplot para la columna numérica actual utilizando seaborn\n",
        "    sns.boxplot(x=df[col])\n",
        "    # Establece el título del gráfico usando el nombre de la columna\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "    # Muestra el gráfico\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPEW7aL3dlj9"
      },
      "source": [
        "## TRATAMIENTO DE VALORES ATÍPICOS (OUTLIERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m23DiqApHHFw"
      },
      "outputs": [],
      "source": [
        "# Instala la librería feature-engine.\n",
        "# --quiet suprime la salida detallada de la instalación.\n",
        "!pip install feature-engine --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db49cc48"
      },
      "outputs": [],
      "source": [
        "# Importa la clase Winsorizer de feature_engine.outliers\n",
        "from feature_engine.outliers import Winsorizer\n",
        "\n",
        "# Crea una instancia de Winsorizer.\n",
        "# capping_method='gaussian' utiliza la media y la desviación estándar para determinar los límites.\n",
        "# tail='right' aplica la winsorización solo al extremo superior (valores atípicos altos).\n",
        "# fold=1.5 define el factor multiplicador de la desviación estándar para calcular el límite (Media + 1.5 * Desviación Estándar).\n",
        "# variables=['price'] especifica que la winsorización se aplicará solo a la columna 'price'.\n",
        "winsorizer = Winsorizer(capping_method='gaussian', tail='right', fold=1.5, variables=['price'])\n",
        "\n",
        "# Aplica la winsorización a la columna 'price' del DataFrame.\n",
        "# fit_transform calcula los límites y aplica la transformación en un solo paso.\n",
        "# Se pasa df[['price']] para asegurarse de que se aplica a un DataFrame (Winsorizer espera un DataFrame).\n",
        "df['price'] = winsorizer.fit_transform(df[['price']])\n",
        "\n",
        "# Imprime un mensaje indicando que se realizó la winsorización en la columna 'price'\n",
        "print(\"Se trataron los valores atípicos en la columna 'price' utilizando Winsorización (el extremo derecho se limita a 1.5 desviaciones estándar de la media).\")\n",
        "# Muestra las primeras filas del DataFrame después de la winsorización para verificar los cambios\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biRNTS3UdsfW"
      },
      "source": [
        "## CORRECCIÓN DE ERRORES TIPOGRÁFICOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9aGR5isdyMW"
      },
      "source": [
        "###Identificación de errores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89b03816"
      },
      "outputs": [],
      "source": [
        "# Define una lista con los nombres de las columnas que podrían contener errores tipográficos (basado en análisis previo)\n",
        "potential_error_cols = ['category_code', 'brand']\n",
        "# Imprime un encabezado indicando las columnas a verificar para errores tipográficos\n",
        "print(\"Posibles columnas para la corrección de errores tipográficos:\")\n",
        "# Itera sobre la lista de columnas potenciales y las imprime\n",
        "for col in potential_error_cols:\n",
        "    print(f\"- {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e6c5995"
      },
      "outputs": [],
      "source": [
        "# Imprime un encabezado indicando que se examinarán los valores únicos en 'category_code'\n",
        "print(\"Examinando los valores únicos en 'category_code' para posibles errores:\")\n",
        "# Muestra los 50 valores únicos más frecuentes y sus conteos en la columna 'category_code'\n",
        "display(df['category_code'].value_counts().head(50))\n",
        "\n",
        "# Imprime un encabezado indicando que se examinarán los valores únicos en 'brand'\n",
        "print(\"\\nExaminando los valores únicos en 'brand' en busca de errores potenciales:\")\n",
        "# Muestra los 50 valores únicos más frecuentes y sus conteos en la columna 'brand'\n",
        "display(df['brand'].value_counts().head(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL-7YdpbeAe-"
      },
      "source": [
        "### Aplicación de correcciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "681090fe"
      },
      "outputs": [],
      "source": [
        "# Define un diccionario para mapear valores incorrectos a valores correctos en 'category_code'\n",
        "category_code_replacements = {\n",
        "    'electronics.smartphone ': 'electronics.smartphone', # Corrige espacio al final\n",
        "    'electronics.telephone': 'electronics.smartphone',   # Mapea 'telephone' a 'smartphone'\n",
        "}\n",
        "\n",
        "# Define un diccionario para mapear valores incorrectos a valores correctos en 'brand'\n",
        "brand_replacements = {\n",
        "    'samsung ': 'samsung', # Corrige espacio al final\n",
        "    'apple ': 'apple',     # Corrige espacio al final\n",
        "    'xiaomi ': 'xiaomi',   # Corrige espacio al final\n",
        "}\n",
        "\n",
        "# Aplica los reemplazos a la columna 'category_code' utilizando el método replace()\n",
        "df['category_code'] = df['category_code'].replace(category_code_replacements)\n",
        "# Aplica los reemplazos a la columna 'brand' utilizando el método replace()\n",
        "df['brand'] = df['brand'].replace(brand_replacements)\n",
        "\n",
        "# Imprime un mensaje indicando que se aplicaron los reemplazos\n",
        "print(\"Se aplicaron reemplazos a las columnas 'category_code' y 'brand'.\")\n",
        "# Muestra las primeras filas del DataFrame después de los reemplazos\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9sG1G4rJ6gE"
      },
      "outputs": [],
      "source": [
        "# Aplica los reemplazos a las categorías de la columna categórica 'category_code'.\n",
        "# rename_categories es más eficiente para columnas de tipo 'category'.\n",
        "# Se usa el diccionario category_code_replacements definido anteriormente.\n",
        "df['category_code'] = df['category_code'].cat.rename_categories(category_code_replacements)\n",
        "# Aplica los reemplazos a las categorías de la columna categórica 'brand'.\n",
        "# Se usa el diccionario brand_replacements definido anteriormente.\n",
        "df['brand'] = df['brand'].cat.rename_categories(brand_replacements)\n",
        "\n",
        "# Imprime un mensaje indicando que se aplicaron los reemplazos usando rename_categories\n",
        "print(\"Se aplicaron reemplazos a las columnas 'category_code' y 'brand' utilizando rename_categories.\")\n",
        "# Muestra las primeras filas del DataFrame después de los reemplazos\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52c4b255"
      },
      "outputs": [],
      "source": [
        "# Define un diccionario de mapeo para category_code.\n",
        "# Este mapeo consolidará 'electronics.telephone' en 'electronics.smartphone'.\n",
        "category_code_mapping = {\n",
        "    'electronics.telephone': 'electronics.smartphone'\n",
        "}\n",
        "\n",
        "# Define un diccionario de mapeo para brand.\n",
        "# Este mapeo consolidará valores con espacios al final.\n",
        "brand_mapping = {\n",
        "    'samsung ': 'samsung',\n",
        "    'apple ': 'apple',\n",
        "    'xiaomi ': 'xiaomi'\n",
        "}\n",
        "\n",
        "# Aplica el mapeo a las categorías de la columna 'category_code' usando rename_categories.\n",
        "# Esto renombra las categorías según el diccionario de mapeo.\n",
        "df['category_code'] = df['category_code'].cat.rename_categories(category_code_mapping)\n",
        "# Imprime un mensaje indicando que se aplicó el mapeo a 'category_code'\n",
        "print(\"Aplicó el método map() a la columna 'category_code'.\")\n",
        "\n",
        "# Aplica el mapeo a las categorías de la columna 'brand' usando rename_categories.\n",
        "# Esto renombra las categorías según el diccionario de mapeo.\n",
        "df['brand'] = df['brand'].cat.rename_categories(brand_mapping)\n",
        "# Imprime un mensaje indicando que se aplicó el mapeo a 'brand'\n",
        "print(\"Se aplicó el map() a la columna 'brand'.\")\n",
        "\n",
        "# Muestra las primeras filas del DataFrame después de aplicar los mapeos\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LmKldm2eKAb"
      },
      "source": [
        "### Verificación de correcciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "303690ba"
      },
      "outputs": [],
      "source": [
        "# Imprime un encabezado para verificar las correcciones en 'category_code'\n",
        "print(\"Verificando correcciones en 'category_code':\")\n",
        "# Muestra los 10 valores únicos más frecuentes en 'category_code' después de las correcciones\n",
        "display(df['category_code'].value_counts().head(10))\n",
        "\n",
        "# Imprime un encabezado para verificar las correcciones en 'brand'\n",
        "print(\"\\nVerificando correcciones en 'brand':\")\n",
        "# Muestra los 10 valores únicos más frecuentes en 'brand' después de las correcciones\n",
        "display(df['brand'].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff9283b5"
      },
      "outputs": [],
      "source": [
        "# Cuenta el número de filas donde 'category_code' es 'electronics.smartphone'.\n",
        "# Esto incluye tanto los valores originales como los corregidos/mapeados a este valor.\n",
        "category_code_corrected_count = df[df['category_code'] == 'electronics.smartphone'].shape[0]\n",
        "# Cuenta el número de filas donde 'brand' es 'samsung'.\n",
        "brand_corrected_count_samsung = df[df['brand'] == 'samsung'].shape[0]\n",
        "# Cuenta el número de filas donde 'brand' es 'apple'.\n",
        "brand_corrected_count_apple = df[df['brand'] == 'apple'].shape[0]\n",
        "# Cuenta el número de filas donde 'brand' es 'xiaomi'.\n",
        "brand_corrected_count_xiaomi = df[df['brand'] == 'xiaomi'].shape[0]\n",
        "\n",
        "# Imprime el conteo de filas con el valor 'electronics.smartphone' después de las correcciones.\n",
        "# Se aclara que incluye filas que ya tenían este valor.\n",
        "print(f\"Valores corregidos (conteo inferido de filas con valor 'from' mapeado): {category_code_corrected_count:,} filas afectadas (incluye filas que ya eran 'electronics.smartphone')\")\n",
        "\n",
        "# Imprime un encabezado para la columna 'brand'\n",
        "print(\"\\nColumna: brand\")\n",
        "\n",
        "# Imprime un encabezado para los conteos corregidos de marcas\n",
        "print(f\"\\nValores corregidos (conteo inferido de filas con valores 'from' mapeados):\\n\")\n",
        "\n",
        "# Imprime el conteo de filas para cada marca después de las correcciones, aclarando que incluye valores originales.\n",
        "print(f\"  - 'samsung' -> 'samsung': {brand_corrected_count_samsung:,} filas afectadas (incluye filas que ya estaban 'samsung')\")\n",
        "print(f\"  - 'apple' -> 'apple': {brand_corrected_count_apple:,} filas afectadas (incluye filas que ya estaban'apple')\")\n",
        "print(f\"  - 'xiaomi' -> 'xiaomi': {brand_corrected_count_xiaomi:,} filas afectadas (incluye filas que ya estaban 'xiaomi')\")\n",
        "# Imprime un resumen de los mapeos realizados en la columna 'brand'.\n",
        "print(\"\\nValores mapeados: 'samsung ' -> 'samsung', 'apple ' -> 'apple', 'xiaomi ' -> 'xiaomi'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu-qpxreeTV1"
      },
      "source": [
        "## ANÁLISIS EXPLORATORIO Y AGREGACIONES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__6y2kEQeVsx"
      },
      "source": [
        "### Conteo de eventos por usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb94c3f6"
      },
      "outputs": [],
      "source": [
        "# Agrupa el DataFrame por 'user_id' y 'event_type'.\n",
        "# Calcula el tamaño de cada grupo (conteo de eventos).\n",
        "# Usa unstack() para pivotar 'event_type' a columnas, con los conteos como valores.\n",
        "# fill_value=0 reemplaza los valores NaN (donde un usuario no tiene un tipo de evento particular) con 0.\n",
        "user_event_counts = df.groupby(['user_id', 'event_type']).size().unstack(fill_value=0)\n",
        "# Imprime un encabezado indicando el contenido del DataFrame resultante\n",
        "print(\"Conteo de eventos por usuario y tipo de evento:\")\n",
        "# Muestra las primeras filas del DataFrame con el conteo de eventos por usuario y tipo\n",
        "display(user_event_counts.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQG8QhCJecAJ"
      },
      "source": [
        "### Análisis de ventas mensuales por categoría"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "858cda77"
      },
      "outputs": [],
      "source": [
        "# Crea una nueva columna 'month' extrayendo el período (Mes) de la columna 'event_time'.\n",
        "# to_period('M') convierte la fecha a un objeto Period representando el mes.\n",
        "df['month'] = df['event_time'].dt.to_period('M')\n",
        "\n",
        "# Filtra el DataFrame para incluir solo eventos de tipo 'purchase'.\n",
        "# Agrupa los resultados por 'month' y 'category_code'.\n",
        "# Suma los precios para obtener las ventas totales por mes y categoría.\n",
        "# observed=True es útil con tipos categóricos para excluir combinaciones no observadas.\n",
        "# reset_index() convierte el resultado de la agrupación en un DataFrame regular.\n",
        "monthly_category_sales = df[df['event_type'] == 'purchase'].groupby(['month', 'category_code'], observed=True)['price'].sum().reset_index()\n",
        "# Imprime un encabezado indicando el contenido del DataFrame resultante\n",
        "print(\"Ventas totales por mes y categoría:\")\n",
        "# Muestra las primeras filas del DataFrame con las ventas mensuales por categoría\n",
        "display(monthly_category_sales.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT327HIEejhO"
      },
      "source": [
        "## EVALUACIÓN FINAL DE LA CALIDAD DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc44c126"
      },
      "outputs": [],
      "source": [
        "# Imprime un encabezado para la evaluación de la completitud de los datos\n",
        "print(\"Evaluación de la Completitud de los Datos\\n\")\n",
        "# Calcula el número de valores nulos por columna después de todas las limpiezas\n",
        "null_counts_final = df.isnull().sum()\n",
        "# Obtiene una lista de nombres de columnas que aún tienen valores nulos\n",
        "cols_with_nulls_final = null_counts_final[null_counts_final > 0].index.tolist()\n",
        "\n",
        "# Verifica si hay columnas con valores nulos restantes\n",
        "if cols_with_nulls_final:\n",
        "    # Imprime un encabezado para las columnas con nulos restantes\n",
        "    print(\"Columnas con valores nulos restantes después del tratamiento:\\n\")\n",
        "    # Itera sobre las columnas con nulos restantes y las imprime con su conteo y porcentaje\n",
        "    for col in cols_with_nulls_final:\n",
        "        count = null_counts_final[col]\n",
        "        print(f\"• {col}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
        "# Si no hay columnas con valores nulos restantes\n",
        "else:\n",
        "    # Imprime un mensaje indicando que no hay valores nulos restantes\n",
        "    print(\"No hay valores nulos en el dataset después del tratamiento.\\n\")\n",
        "\n",
        "# Define una lista de columnas consideradas críticas para el análisis\n",
        "critical_cols = ['event_time', 'event_type', 'product_id', 'user_id', 'user_session', 'price']\n",
        "# Calcula el número total de valores nulos en las columnas críticas\n",
        "missing_in_critical_cols = null_counts_final[critical_cols].sum()\n",
        "\n",
        "# Verifica si hay valores nulos en las columnas críticas\n",
        "if missing_in_critical_cols == 0:\n",
        "    # Imprime un mensaje indicando que no hay nulos en las columnas críticas\n",
        "    print(\"No hay valores nulos en las columnas críticas.\")\n",
        "else:\n",
        "    # Imprime una advertencia si se encontraron nulos en columnas críticas\n",
        "    print(\"\\nAdvertencia: Se encontraron valores nulos en las siguientes columnas críticas:\\n\")\n",
        "    # Itera sobre las columnas críticas y, si tienen nulos, imprime su conteo\n",
        "    for col in critical_cols:\n",
        "        if null_counts_final[col] > 0:\n",
        "            print(f\"• {col}: {null_counts_final[col]:,} nulos\")\n",
        "\n",
        "# Imprime el número total de filas después de la limpieza\n",
        "print(f\"\\nNúmero total de filas después de la limpieza: {len(df):,}\")\n",
        "# Imprime el número total de filas iniciales (antes de eliminar duplicados/nulos)\n",
        "print(f\"Número total de filas iniciales: {initial_rows:,}\")\n",
        "# Compara el número de filas final con el inicial para ver cuántas se eliminaron\n",
        "if len(df) < initial_rows:\n",
        "    # Imprime cuántas filas se eliminaron y por qué\n",
        "    print(f\"Se eliminaron {initial_rows - len(df):,} filas (principalmente duplicados y filas con user_session nulo).\")\n",
        "    print(\"Esto es esperado y no representa una pérdida de datos críticos para la mayoría de los análisis, ya que se eliminaron registros no válidos o redundantes.\")\n",
        "else:\n",
        "    # Imprime un mensaje si no se eliminaron filas\n",
        "    print(\"No se eliminaron filas durante la limpieza.\")\n",
        "\n",
        "# Imprime un encabezado para la evaluación de la relevancia de las variables\n",
        "print(\"\\nEvaluación de la Relevancia de las Variables\\n\")\n",
        "# Imprime un encabezado para listar las columnas disponibles\n",
        "print(\"Columnas disponibles en el dataset:\\n\")\n",
        "# Itera sobre las columnas del DataFrame y las imprime con su tipo de dato\n",
        "for col in df.columns:\n",
        "    print(f\"- {col} (Tipo: {df[col].dtype})\")\n",
        "\n",
        "# Comenta sobre la relevancia general de las columnas para el análisis de e-commerce\n",
        "print(\"\\nLas columnas presentes (event_time, event_type, product_id, category_id, category_code, brand, price, user_id, user_session, month) son relevantes para los objetivos de análisis de comportamiento de e-commerce.\")\n",
        "\n",
        "# Imprime un encabezado para la evaluación de la granularidad\n",
        "print(\"\\nEvaluación de la Granularidad\\n\")\n",
        "# Describe la granularidad del dataset a nivel de evento\n",
        "print(f\"La granularidad del dataset es a nivel de evento ({len(df):,} eventos individuales). Cada fila representa una acción específica de un usuario en un momento dado.\")\n",
        "print(\"Esta granularidad es adecuada para análisis detallados como embudos de conversión, secuencias de eventos por usuario/sesión, análisis de productos/categorías a nivel de evento, y análisis de precios por evento.\")\n",
        "print(\"Se pueden realizar agregaciones (por ejemplo, por usuario, sesión, producto, categoría, tiempo) para análisis con granularidad menor según sea necesario.\")\n",
        "\n",
        "# Imprime un encabezado para la conclusión de la evaluación\n",
        "print(\"\\nConclusión de la Evaluación\\n\")\n",
        "# Resume la conclusión basada en si quedan valores nulos o no\n",
        "if not cols_with_nulls_final:\n",
        "    print(\"El dataset está libre de valores nulos después del tratamiento.\")\n",
        "    print(\"Las columnas presentes son relevantes y la granularidad a nivel de evento permite realizar análisis detallados y agregados para cumplir los objetivos iniciales.\")\n",
        "    print(\"El dataset limpio parece adecuado para proceder con los análisis.\")\n",
        "else:\n",
        "    print(\"Advertencia: Todavía hay columnas con valores nulos. Dependiendo de los objetivos específicos de análisis, puede ser necesario un tratamiento adicional para estas columnas.\")\n",
        "    print(\"Sin embargo, las columnas críticas para la mayoría de los análisis de comportamiento parecen estar completas.\")\n",
        "    print(\"Las columnas presentes son relevantes y la granularidad es adecuada para análisis detallados y agregados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opKuthSWetgp"
      },
      "source": [
        "## GUARDADO DEL DATASET LIMPIO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b17ef220"
      },
      "source": [
        "# Instalar la biblioteca xlsxwriter\n",
        "%pip install xlsxwriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1268040"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def save_depurado(df, path=\"/content/drive/MyDrive/ProyectoIntegradoIII_Analítica_de_Datos_PREICA2501B020071\"):\n",
        "    \"\"\"Guarda DataFrame rápido en CSV y Excel\"\"\"\n",
        "\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    csv_file = f\"{path}/depurado.csv\"\n",
        "    excel_file = f\"{path}/depurado.xlsx\"\n",
        "\n",
        "    print(f\"💾 Guardando {len(df):,} filas...\")\n",
        "    start = time.time()\n",
        "\n",
        "    # CSV rápido\n",
        "    df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    # Excel optimizado\n",
        "    # Create a copy of the DataFrame to modify for Excel\n",
        "    df_excel = df.copy()\n",
        "    # Convert timezone-aware datetime columns to timezone-naive for Excel\n",
        "    for col in df_excel.select_dtypes(include=['datetime64[ns, UTC]']).columns:\n",
        "        df_excel[col] = df_excel[col].dt.tz_convert(None)\n",
        "\n",
        "\n",
        "    with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
        "        if len(df_excel) > 50000:\n",
        "            # Dividir en hojas si es muy grande\n",
        "            for i in range(0, len(df_excel), 50000):\n",
        "                chunk = df_excel.iloc[i:i+50000]\n",
        "                # Ensure column names are strings\n",
        "                chunk.columns = chunk.columns.astype(str)\n",
        "                chunk.to_excel(writer, sheet_name=f'Datos_{i//50000+1}', index=False)\n",
        "        else:\n",
        "            # Ensure column names are strings\n",
        "            df_excel.columns = df_excel.columns.astype(str)\n",
        "            df_excel.to_excel(writer, sheet_name='Datos', index=False)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    csv_mb = os.path.getsize(csv_file) / (1024**2)\n",
        "    excel_mb = os.path.getsize(excel_file) / (1024**2)\n",
        "\n",
        "    print(f\"✅ Completado en {elapsed:.1f}s\")\n",
        "    print(f\"📄 CSV: {csv_mb:.1f}MB\")\n",
        "    print(f\"📊 Excel: {excel_mb:.1f}MB\")\n",
        "\n",
        "    return {'csv': csv_file, 'excel': excel_file}\n",
        "\n",
        "# Uso simple:\n",
        "archivos = save_depurado(df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fttbMjUz41c83qzw1YK7773nGAWMyHAQ",
      "authorship_tag": "ABX9TyPIUmDvGg4DET3Tf8Nctaa+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}